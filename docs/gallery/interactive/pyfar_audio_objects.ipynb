{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with pyfar Audio objects\n",
    "\n",
    "Audio data are at the core or pyfar and three classes can be used for storing, processing, and visualizing such data. There are three different audio objects\n",
    "\n",
    "- `pyfar.Signal` can be used to store equidistant and complete signals that can be converted between the time and frequency domain.\n",
    "- `pyfar.TimeData` and `pyfar.FrequencyData` are intended data that can not be converted between the time and frequency domain. Examples for this are time data with missing data, frequency data that is only available at certain frequencies, or an energy histogram.\n",
    "\n",
    "The following examples introduce fundamental concepts behind audio classes and show how the data inside audio objects can be accessed. See the `pyfar documentation` for a complete overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyfar as pf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating audio objects\n",
    "\n",
    "Creating an audio object is similar across classes. To create a `TimeData` object, specify the data and the times at which the data was sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time data with non-equidistant sampling times\n",
    "time = pf.TimeData([1, 0, -1], [0, 1, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a `FrequencyData` object requires the specification of the frequencies that belong to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# band limited frequency data object\n",
    "frequency = pf.FrequencyData([1, .8, .7], [400, 800, 1600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Signal` object can be created in the time or frequency data and requires the sampling rate of the signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create signal object with time data and a sampling rate of 4 Hz\n",
    "signal = pf.Signal([1, 0, 0, 0], 4)\n",
    "\n",
    "# creating a signal object in the frequency domain requires additional\n",
    "# parameters\n",
    "signal2 = pf.Signal([1, 1, 1], 4, n_samples=4, domain='freq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing data in audio objects\n",
    "\n",
    "The remainder of the examples focuses on Signal objects as the most common audio type, but there are many analogies between Signal and Time/FrequencyData objects. So using other kinds of audio objects will be fairly simple after understanding Signal objects.\n",
    "\n",
    "The data stored in audio objects can be accessed trough their `time` and `freq` properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "returns the time data and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal2.freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "returns the frequency data. There are a few important things to note right away.\n",
    "\n",
    "- for convenience, the audio data is stored in `numpy` arrays\n",
    "- data inside audio objects is at least 2-dimensional. In this example `signal` is an audio object with a single channel of audio data and the shape of `signal.time` is `(1, 4)`. In pyfar the samples of time data (4 in this example) and the frequency bins of frequency data are always stored in the last dimension of `signal.time` and `signal.freq`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, the `time` property can be used to access data of `TimeData` objects and the `freq` property to access data of `FrequencyData` objects. For `Signal` objects time and frequency data can be accessed at any time and the Signal class converts between the time and frequency domain if required using the Discrete Fourier Transform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing frequency data from the signal that was created with time data\n",
    "signal.freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it is **important to note** that the data returned by the `freq` property can depend on the normalization of the Discrete Fourier Transform as explained in more detail `here`. The frequency data without any normalization can be accessed using the `freq_raw` property. However, for this example the two return the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.freq_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read on to learn how to find out which normalization of the Discrete Fourier Transform is used for a signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting data in audio objects\n",
    "\n",
    "The same properties can be used to set the data inside audio objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.time = [2, 0, 0, 0]\n",
    "print(f'signal.time = {signal.time}')\n",
    "signal2.freq = [2, 2, 2]\n",
    "print(f'signal2.freq = {signal2.freq}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional information stored in audio objects\n",
    "\n",
    "Converting between time and frequency data is already useful, but audio objects can do more. They contain additional information about their data that can be helpful. Some basic information is already returned when prompting an audio object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets break down the information one by one. \n",
    "\n",
    "**Domain:** The signal is in the time domain, because we last accessed its time data above. We can get and set this property with the `signal.domain` attribute but it is of not much interested when working with Signal objects because they are automatically converted between the time and frequency domain, when it is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Signal Type:** It is an *energy* Signal, which is important to note and understand. Energy signals are finite signals with finite energy. Examples for these kinds of signals are impulse responses. *Power* signals on the other hand have an infinite duration and energy. Examples are noise or sine signals of which we typically observe a block of finite size. The signal type is a read only property that is set by the DFT normalization introduced below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.signal_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DFT Normlaization:** For *power* Signals, different DFT normalization can be used that differently scale the values stored in `signal.freq`. For *energy* signals, the DFT normalization is always 'none'. For more information refer the `Fast Fourier Transform` examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.fft_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal cshape, length, and caxis\n",
    "\n",
    "Signals can contain multiple channels of audio data and the shape of the data inside the audio object is often important. There multiple attributes describing this.\n",
    "\n",
    "**Channel shape:** The *channel shape*, in short `cshape` gives the shape of the data inside an audio object but ignores the number of samples or frequency bins. For example our first Signal created above has one channel and 4 samples, hence the data is of chape `(1, 4)`. Because the `cshape` ignores the samples it is `(1, )`. There are two ideas behind this. First, digital signal processing methods often work on channels, and second the length of the signal might be different in the time and frequency domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.cshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Length:** The signal length in samples and bins is stored in the properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.n_bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that only the half sided spectrum is stored for real valued time signals. Hence the length differs in the time and frequency domain in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Channel axis:** Some functions in pyfar operate along one or more axes of the data. In analogy, to the channel chape, these axes are referred to as *channel axis* or in short `caxis`. Consider a signal of `cshape=(3, 4)` and a duration of `n_samples=128`. In this case `caxis=0` referst to the first dimension of size 3 and `caxis=-1` referns to the last dimension of size 4 but *not* dimension containing the 128 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multidimensional audio objects\n",
    "\n",
    "Audio objects may contain data in multiple channels and pyfar offers some functionality to ease handling such signals. Let the following signal of `cshape=(2, 3)` symbolize data obtained for t2 loudspeaker and 3 microphone positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal3 = pf.Signal(\n",
    "    [[[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0]],\n",
    "     [[0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]]],\n",
    "    6)\n",
    "\n",
    "signal3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A subset of the channels can be accessed through slicing. For example the first channel in both dimensions can be obtained by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = signal3[0, 0]\n",
    "\n",
    "channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as the output tells us this returns a signal object, which is often useful because most pyfar functions require signal objects as input. This also works for obtaining larger slices of the signal, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = signal3[0]\n",
    "channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reading\n",
    "\n",
    "After getting used to adding and accessing data in audio objects, you are ready for discovering ways of inspecting and working with audio objects. Good next steps might be learning how to graphically display (plot) data from audio objects as detailed `here` and check out how to apply simple arithmetic operations with audio objects as described `here`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyfar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
