{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binaural synthesis\n",
    "Humans can localize sound events about their perceived distance and their angular position in space.\n",
    "Sound waves are altered due to reflections, diffraction, and resonances caused by the presence of a human body, head, shoulders, torso, and the fine structure of the ear formed by pinna, cavum conchae, etc.\n",
    "All these effects, which in its assembly are evaluated by the human brain to localize a source or to get other spatial information, are integrated into binaural signals.\n",
    "If the binaural signal is reproduced perfectly at the eardrums (the human microphones), there is no chance to acoustically distinguish the virtual source or environment from the real sound field.\n",
    "With binaural synthesis, a filtering approach with special filters, an acoustic sound source represented by a mono-signal can be virtually placed at arbitrary space positions.\n",
    "\n",
    "## HRTF dataset\n",
    "\n",
    "A valid way to describe all linear sound transformations caused by torso, head, and pinna is using so-called “head-related transfer functions” (HRTFs).\n",
    "For each direction of sound incidence from a sound source to a human receiver, two transfer functions exist (one for the left and one for the right ear), combined into a two-channel HRTF in the frequency domain.\n",
    "Combining all directions into a single database is commonly called an HRTF dataset.\n",
    "The time domain version of HRTFs are also known as \"head-related impulse responses\" (HRIRs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# import the required packages\n",
    "import pyfar as pf\n",
    "import sofar as sf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "files = pf.signals.files._load_files('head_related_impulse_responses')\n",
    "hrir_file = os.path.join(pf.signals.files.file_dir, files[0])\n",
    "\n",
    "# load a SOFA file containing the HRIR dataset\n",
    "hrirs, sources, _ = pf.io.read_sofa(hrir_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are loading the included HRIR dataset from the FABIAN dummy head by [Brinkmann _et al._](https://depositonce.tu-berlin.de/items/3b423df7-a764-4ce1-9065-4e6034bba759).\n",
    "`pyfar` includes a [method to load specific HRIRs from the dataset](https://pyfar.readthedocs.io/en/stable/modules/pyfar.signals.files.html#pyfar.signals.files.head_related_impulse_responses) but the example shown here is the general approach for loading a SOFA file.\n",
    "\n",
    "First, we will plot all possible source locations that are contained in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the included dataset only contains a limited number of source positions - namely, the horizontal plane and the median plane.\n",
    "In practice, an HRIR dataset usually contains more source positions covering the whole sphere around the listener.\n",
    "\n",
    "Using the `pyfar` coordinates, <!-- LINK TODO --> a specific direction can be selected from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the direction of the desired sound source; here, we choose the right direction\n",
    "elevation = 0\n",
    "azimuth = -90\n",
    "\n",
    "# convert to radians\n",
    "elevation = elevation * np.pi / 180\n",
    "azimuth = azimuth * np.pi / 180\n",
    "\n",
    "desired_direction = pf.Coordinates.from_spherical_elevation(\n",
    "    azimuth, elevation, 2\n",
    ")\n",
    "\n",
    "index, _ = sources.find_nearest(desired_direction)\n",
    "\n",
    "sources.show(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot, the selected direction is marked with a red dot.\n",
    "\n",
    "With an HRIR selected, we can plot it in the time and frequency domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pf.plot.time_freq(hrirs[index], label=[\"Left ear\", \"Right ear\"])\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a binaural synthesis\n",
    "\n",
    "The binaural synthesis is done by convolving a mono, anechoic signal with an HRIR.\n",
    "The following code shows how to do this with `pyfar`.\n",
    "We will create a function for this to reuse it later.\n",
    "This function gets the desired azimuth direction and selects the corresponding HRIR from the dataset.\n",
    "Then, it convolves the input signal with the HRIR and returns the binaural signal.\n",
    "As an additional functionality, this function also allows a gain to be applied to the binaural signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve_with_hrir(azimuth, audio, hrirs, gain=1.0):\n",
    "    # convert to radians\n",
    "    azimuth = azimuth * np.pi / 180\n",
    "\n",
    "    # define the direction of the desired sound source\n",
    "    desired_direction = pf.Coordinates.from_spherical_elevation(\n",
    "        azimuth, 0, 2\n",
    "    )\n",
    "\n",
    "    # find the nearest HRIRs to the desired direction\n",
    "    index, _ = sources.find_nearest(desired_direction)\n",
    "\n",
    "    # apply the HRIRs to the audio signal\n",
    "    output_audio = pf.dsp.convolve(audio, hrirs[index])\n",
    "    output_audio = 10 ** (gain / 20) * pf.dsp.normalize(output_audio)\n",
    "\n",
    "    return output_audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use one of the example audio files included in `pyfar` as an input signal.\n",
    "\n",
    "The following code loads the audio signal, plots the time domain and allows you to listen to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "castanets = pf.signals.files.castanets()\n",
    "\n",
    "pf.plot.time(castanets)\n",
    "\n",
    "Audio(castanets.time, rate=castanets.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a binaural synthesis from this signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binaural_audio = convolve_with_hrir(-90, castanets, hrirs)\n",
    "\n",
    "Audio(binaural_audio.time, rate=binaural_audio.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive example\n",
    "\n",
    "Last but not least, we will create an interactive example that allows you to select a source position and listen to the binaural synthesis.\n",
    "In addition, we will also use two signals to demonstrate the effect of the binaural synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ipywidgets import (\n",
    "#     GridspecLayout,\n",
    "#     Button,\n",
    "#     Layout,\n",
    "#     IntSlider,\n",
    "#     interactive_output,\n",
    "# )\n",
    "\n",
    "# guitar = pf.signals.files.guitar(44100)\n",
    "# guitar.time = guitar.time[:,:castanets.n_samples]\n",
    "\n",
    "\n",
    "# def interactive_demo(castanets_azimuth, castanets_gain, guitar_azimuth, guitar_gain):\n",
    "#     castanets_audio = convolve_with_hrir(castanets_azimuth, castanets, hrirs, castanets_gain)\n",
    "#     guitar_audio = convolve_with_hrir(guitar_azimuth, guitar, hrirs, guitar_gain)\n",
    "\n",
    "#     mixed_audio = pf.classes.audio.add((castanets_audio, guitar_audio), domain=\"time\")\n",
    "\n",
    "#     mixed_audio = pf.dsp.normalize(mixed_audio)\n",
    "\n",
    "#     display(Audio(mixed_audio.time, rate=mixed_audio.sampling_rate, normalize=False))\n",
    "\n",
    "# castanets_btn = Button(\n",
    "#     description=\"Castanets\",\n",
    "#     button_style=\"success\",\n",
    "#     layout=Layout(height=\"auto\", width=\"auto\"),\n",
    "# )\n",
    "# guitar_btn = Button(\n",
    "#     description=\"Guitar\",\n",
    "#     button_style=\"success\",\n",
    "#     layout=Layout(height=\"auto\", width=\"auto\"),\n",
    "# )\n",
    "# castanets_az_sl = IntSlider(\n",
    "#     value=90,\n",
    "#     min=-180,\n",
    "#     max=180,\n",
    "#     step=5,\n",
    "#     description=\"Azimuth [deg]\",\n",
    "#     continuous_update=False,\n",
    "#     layout=Layout(height=\"auto\", width=\"auto\"),\n",
    "# )\n",
    "# castanets_gain_sl = IntSlider(\n",
    "#     value=0,\n",
    "#     min=-50,\n",
    "#     max=0,\n",
    "#     step=1,\n",
    "#     description=\"Gain [dB]\",\n",
    "#     continuous_update=False,\n",
    "#     layout=Layout(height=\"auto\", width=\"auto\"),\n",
    "# )\n",
    "# guitar_az_sl = IntSlider(\n",
    "#     value=-90,\n",
    "#     min=-180,\n",
    "#     max=180,\n",
    "#     step=5,\n",
    "#     description=\"Azimuth [deg]\",\n",
    "#     continuous_update=False,\n",
    "#     layout=Layout(height=\"auto\", width=\"auto\"),\n",
    "# )\n",
    "# guitar_gain_sl = IntSlider(\n",
    "#     value=0,\n",
    "#     min=-50,\n",
    "#     max=0,\n",
    "#     step=1,\n",
    "#     description=\"Gain [dB]\",\n",
    "#     continuous_update=False,\n",
    "#     layout=Layout(height=\"auto\", width=\"auto\"),\n",
    "# )\n",
    "\n",
    "# grid = GridspecLayout(3, 2, height=\"200px\")\n",
    "# grid[0, 0] = castanets_btn\n",
    "# grid[1, 0] = castanets_az_sl\n",
    "# grid[2, 0] = castanets_gain_sl\n",
    "# grid[0, 1] = guitar_btn\n",
    "# grid[1, 1] = guitar_az_sl\n",
    "# grid[2, 1] = guitar_gain_sl\n",
    "\n",
    "# out = interactive_output(\n",
    "#     interactive_demo,\n",
    "#     {\n",
    "#         \"castanets_azimuth\": castanets_az_sl,\n",
    "#         \"castanets_gain\": castanets_gain_sl,\n",
    "#         \"guitar_azimuth\": guitar_az_sl,\n",
    "#         \"guitar_gain\": guitar_gain_sl,\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# display(grid, out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
